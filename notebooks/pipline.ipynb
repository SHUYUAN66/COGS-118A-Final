{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Sets\n",
    "and some piplines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from pprintpp import pprint\n",
    "import timeit \n",
    "\n",
    "# preprocessing\n",
    "    # StandardScaler(), KFold()\n",
    "from sklearn.preprocessing import  StandardScaler,  OneHotEncoder\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer \n",
    "# selecting model  \n",
    "from sklearn.model_selection import train_test_split,KFold,GridSearchCV,validation_curve, learning_curve\n",
    "    # Algorithms\n",
    "from supervised.automl import AutoML\n",
    "from sklearn.utils.fixes import loguniform\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "models = []\n",
    "models.append(('knn', KNeighborsClassifier()))\n",
    "models.append(('dtree', DecisionTreeClassifier()))\n",
    "models.append(('svm', SVC()))\n",
    "# evaluation\n",
    "from sklearn.metrics import f1_score, accuracy_score,roc_curve, confusion_matrix,classification_report\n",
    "\n",
    "\"\"\"\n",
    "* The DataFrame: \n",
    "    group1: ACC, FSC, LFT\n",
    "    group2: ROC, APR, BEP\n",
    "    group3: RMS, MXE\n",
    "    group4: OPT-SELS\n",
    "    - The **threshold** metrics are accuracy(ACC), F-score(FSC) and lift(LFT).\n",
    "    - The **rank metrics** we use are area under the ROC curve (ROC),average precision(APR), and precision/recall break even point (BEP).\n",
    "    - The **probability metrics**,squared error(RMS) and cross-entropy (MXE),interpret the predicted value of each case as the conditional probability of that case being in the positive class.\n",
    "    - The last column,***OPT-SEL***,is the mean normalized score for the eight metrics when model selection is done by cheating and looking at the final testsets.(dont understand either.....)\n",
    "\n",
    "* Within the Matric: \n",
    "    - The algorithm with the best performance on each metric is ***boldfaced***. Using t-test p= 0.05, others are *'ed in three trails if they still have good performance.\n",
    "    \n",
    "    \"\"\"\n",
    "seed = 7\n",
    "results = []\n",
    "names = []\n",
    "prec, rec = 1, 1\n",
    "def bep(y_true, y_pred):\n",
    "    precision()\n",
    "    return precision/recall\n",
    "scorings = {\"ACC\": 'accuracy', \n",
    "            'FSC': 'f1', \n",
    "            'LFT': 'lift',\n",
    "            # ROC, APR, BEP\n",
    "            'ROC': 'roc_auc',\n",
    "            \"APR\" : 'average_precision',\n",
    "            \"BEP\" : bep(prec, rec),\n",
    "            # RMS, MXE\n",
    "            'RMS': 'mean_squared_error',\n",
    "            'MXE': 'max_error',\n",
    "            #OPT-SELS\n",
    "            'OPT-SELS': 'mm'}\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url1 = \"https://archive.ics.uci.edu/ml/datasets/Adult\"\n",
    "url2 = 'https://archive.ics.uci.edu/ml/datasets/Avila'\n",
    "url3 = 'https://archive.ics.uci.edu/ml/datasets/Nursery'\n",
    "url4 = 'http://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption'\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Dataset Information\n",
    "1. ADULT\n",
    "    * TARGET: >50K, <=50K.\n",
    "    * VARIABLES/ ATTRIBUTES: ['age','workclass','fnlwgt','education',\n",
    "    'education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country']\n",
    "2.  AVILA\n",
    "    * TARGET: {Class: A, B, C, D, E, F, G, H, I, W, X, Y}\n",
    "    * VARIABLES/ ATTRIBUTES: ['0', '1', '2', '3', '4', '5', '6', '7', '8', \n",
    "    '9']\n",
    "3. NURSERY\n",
    "    * TARGET: NURSERY\n",
    "    * VARIABLES/ ATTRIBUTES: nry_var = ['parents', 'has_nurs', 'form', 'children', 'housing','finance', 'social', 'health']\n",
    "4. HOUSEHOLD POWER \n",
    "    * Time series\n",
    "    * VARIABLES/ ATTRIBUTES: ['Date', 'Time', 'Global_active_power', 'Global_reactive_power',\n",
    "       'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2',\n",
    "       'Sub_metering_3']\n",
    "    * For EC! \n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "get_df_name(df): \n",
    "    input: a dataframe.\n",
    "    output: the name of this dataframe.\n",
    "check_dataset(dataset): \n",
    "    goal: make sure fulfill the requirements of the dataset choice.\n",
    "    input: a list of dataframes.\n",
    "    output: print out the size information of each dataframe.\n",
    "check_sample(dataset):\n",
    "    goal: make sure choosing 5000 samples randomly.\n",
    "    input: a list of dataframes:\n",
    "    output: print out the size of information of each sampled dataframe.\n",
    "'''\n",
    "\n",
    "def get_df_name(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    return name\n",
    "\n",
    "def check_dataset(dataset):\n",
    "    for i in dataset:\n",
    "        if i.shape[0]>=10000:\n",
    "            print('Dataset '+ get_df_name(i)+\" fulfill the size requirements in the project having \"+ str(i.shape[0])+\" samples, which is over or equal to 10,000\")\n",
    "    return\n",
    "\n",
    "def check_sample(dataset):\n",
    "    for i in dataset:\n",
    "        if i.shape[0]==5000:\n",
    "            print('Successfully randomly choosing 5000 data samples from ' +get_df_name(i)+\" dataset\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start on reading raw data\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Finish reading 4 datasets \n",
      "\n",
      "-\n",
      "Checking Dataset Size of datasets\n",
      " .......\n",
      "Dataset avlang fulfill the size requirements in the project having 10430 samples, which is over or equal to 10,000\n",
      "Dataset adult_raw fulfill the size requirements in the project having 32561 samples, which is over or equal to 10,000\n",
      "Dataset house_raw fulfill the size requirements in the project having 2075259 samples, which is over or equal to 10,000\n",
      "Dataset nursey_raw fulfill the size requirements in the project having 12960 samples, which is over or equal to 10,000\n",
      " .......\n",
      "Finish checking dataset\n",
      "\n",
      "\n",
      ".....StartRandome Sampling......\n",
      "Start sampling avlang dataset\n",
      " Finished\n",
      "Start sampling adult dataset\n",
      " Finished\n",
      "Start sampling house_income dataset\n",
      " Finished\n",
      "Start sampling nursery dataset\n",
      " Finished\n",
      "......\n",
      "Finish randome sampling, waiting for check\n",
      "Please continue to run.\n"
     ]
    }
   ],
   "source": [
    "print('Start on reading raw data')\n",
    "avlang = pd.read_csv('../data/train/avl_set/avila-tr.txt', header=None)\n",
    "print('-')\n",
    "adult_raw = pd.read_csv('../data/train/adult/adult.data', header=None)\n",
    "print('-')\n",
    "house_raw = pd.read_csv('../data/train/house_set/household_power_consumption.txt',sep=';')\n",
    "print('-')\n",
    "nursey_raw = pd.read_csv('../data/train/nursey/nursery.data',header=None)\n",
    "print('-')\n",
    "data_set = [avlang,adult_raw,house_raw,nursey_raw]\n",
    "print('Finish reading 4 datasets ')\n",
    "print('')\n",
    "print('-')\n",
    "print('Checking Dataset Size of datasets')\n",
    "print(\" .......\")\n",
    "check_dataset(data_set)\n",
    "print(\" .......\")\n",
    "print('Finish checking dataset')\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\".....StartRandome Sampling......\")\n",
    "print('Start sampling avlang dataset')\n",
    "avlang.sample(n=5000, replace=False).to_csv('./data/avl.csv', index=False)\n",
    "print(' Finished')\n",
    "print('Start sampling adult dataset')\n",
    "adult_raw.sample(n=5000, replace=False).to_csv('./data/adult.csv', index=False)\n",
    "print(' Finished')\n",
    "print('Start sampling house_income dataset')\n",
    "house_raw.sample(n=5000, replace=False).to_csv('./data/income.csv', index=False)\n",
    "print(' Finished')\n",
    "print('Start sampling nursery dataset')\n",
    "nursey_raw.sample(n=5000, replace=False).to_csv('./data/nsr.csv', index=False)\n",
    "print(' Finished')\n",
    "print(\"......\")\n",
    "print('Finish randome sampling, waiting for check')\n",
    "print('Please continue to run.')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start checking size of training dataset\n ......\nSuccessfully randomly choosing 5000 data samples from avl dataset\nSuccessfully randomly choosing 5000 data samples from ad dataset\nSuccessfully randomly choosing 5000 data samples from pwr dataset\nSuccessfully randomly choosing 5000 data samples from nsr dataset\n .....\nFinish checking training dataset\nSizes for All training samples dataset are : (5000, 11) (5000, 15) (5000, 9) (5000, 9)\n"
     ]
    }
   ],
   "source": [
    "print('Start checking size of training dataset')\n",
    "print(' ......')\n",
    "avl = pd.read_csv('./data/avl.csv')\n",
    "ad = pd.read_csv('./data/adult.csv')\n",
    "pwr = pd.read_csv('./data/income.csv')\n",
    "nsr = pd.read_csv('./data/nsr.csv')\n",
    "\n",
    "training = [avl, ad, pwr, nsr]\n",
    "check_sample(training)\n",
    "print(\" .....\")\n",
    "print('Finish checking training dataset')\n",
    "print('Sizes for All training samples dataset are :', avl.shape, ad.shape, pwr.shape, nsr.shape)"
   ]
  },
  {
   "source": [
    "## EDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Add missing variable names\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "   age     workclass  fnlwgt      education  education-num  \\\n0   40   Federal-gov  544792   Some-college             10   \n1   54       Private  171924        HS-grad              9   \n2   23       Private  133061        HS-grad              9   \n3   18       Private  353358   Some-college             10   \n4   19       Private  164395        HS-grad              9   \n\n        marital-status         occupation relationship    race      sex  \\\n0   Married-civ-spouse    Farming-fishing      Husband   White     Male   \n1   Married-civ-spouse   Transport-moving      Husband   White     Male   \n2   Married-civ-spouse              Sales      Husband   White     Male   \n3        Never-married      Other-service    Own-child   White     Male   \n4        Never-married       Adm-clerical    Own-child   White   Female   \n\n   capital-gain  capital-loss  hours-per-week  native-country  target  \n0             0             0              40   United-States   <=50K  \n1             0             0              70   United-States   <=50K  \n2             0             0              80   United-States   <=50K  \n3             0             0              16   United-States   <=50K  \n4             0             0              25   United-States   <=50K  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>workclass</th>\n      <th>fnlwgt</th>\n      <th>education</th>\n      <th>education-num</th>\n      <th>marital-status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>capital-gain</th>\n      <th>capital-loss</th>\n      <th>hours-per-week</th>\n      <th>native-country</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>Federal-gov</td>\n      <td>544792</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Married-civ-spouse</td>\n      <td>Farming-fishing</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>54</td>\n      <td>Private</td>\n      <td>171924</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Transport-moving</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>70</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>23</td>\n      <td>Private</td>\n      <td>133061</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Married-civ-spouse</td>\n      <td>Sales</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>80</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>18</td>\n      <td>Private</td>\n      <td>353358</td>\n      <td>Some-college</td>\n      <td>10</td>\n      <td>Never-married</td>\n      <td>Other-service</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>0</td>\n      <td>0</td>\n      <td>16</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>Private</td>\n      <td>164395</td>\n      <td>HS-grad</td>\n      <td>9</td>\n      <td>Never-married</td>\n      <td>Adm-clerical</td>\n      <td>Own-child</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25</td>\n      <td>United-States</td>\n      <td>&lt;=50K</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "          0         1         2         3         4         5         6  \\\n0  0.229043 -0.016448  0.317424 -0.526838  0.261718 -0.058835  0.635431   \n1  0.006853 -0.181323  0.388552  0.301377 -0.095795  0.730137  0.371178   \n2  0.290762  0.077766  0.118265  0.275805  0.261718  0.107265  0.144676   \n3  0.031541 -0.040001  0.175168 -0.107256  0.261718  2.515706  0.748682   \n4  0.599358 -0.377602  0.242739 -0.151028  0.976743 -1.221530 -1.138838   \n\n          7         8         9 target  \n0 -0.561067 -0.185368 -0.268865      F  \n1  0.265990  0.219991  0.572511      A  \n2  0.217996 -0.341275  0.150750      F  \n3 -1.468097 -0.746634  1.891487      F  \n4  1.187639  1.560794 -0.404919      I  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.229043</td>\n      <td>-0.016448</td>\n      <td>0.317424</td>\n      <td>-0.526838</td>\n      <td>0.261718</td>\n      <td>-0.058835</td>\n      <td>0.635431</td>\n      <td>-0.561067</td>\n      <td>-0.185368</td>\n      <td>-0.268865</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.006853</td>\n      <td>-0.181323</td>\n      <td>0.388552</td>\n      <td>0.301377</td>\n      <td>-0.095795</td>\n      <td>0.730137</td>\n      <td>0.371178</td>\n      <td>0.265990</td>\n      <td>0.219991</td>\n      <td>0.572511</td>\n      <td>A</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.290762</td>\n      <td>0.077766</td>\n      <td>0.118265</td>\n      <td>0.275805</td>\n      <td>0.261718</td>\n      <td>0.107265</td>\n      <td>0.144676</td>\n      <td>0.217996</td>\n      <td>-0.341275</td>\n      <td>0.150750</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.031541</td>\n      <td>-0.040001</td>\n      <td>0.175168</td>\n      <td>-0.107256</td>\n      <td>0.261718</td>\n      <td>2.515706</td>\n      <td>0.748682</td>\n      <td>-1.468097</td>\n      <td>-0.746634</td>\n      <td>1.891487</td>\n      <td>F</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.599358</td>\n      <td>-0.377602</td>\n      <td>0.242739</td>\n      <td>-0.151028</td>\n      <td>0.976743</td>\n      <td>-1.221530</td>\n      <td>-1.138838</td>\n      <td>1.187639</td>\n      <td>1.560794</td>\n      <td>-0.404919</td>\n      <td>I</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "         Date      Time Global_active_power Global_reactive_power   target  \\\n0   25/1/2010  10:47:00               1.444                 0.088  242.600   \n1  14/11/2007  10:54:00               1.360                 0.000  237.830   \n2   21/1/2007  14:19:00               1.758                 0.406  243.950   \n3    9/1/2009  18:14:00               3.358                 0.000  239.730   \n4   17/6/2009  23:43:00               0.428                 0.076  242.830   \n\n  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n0            5.800          0.000          0.000            19.0  \n1            5.600          0.000          0.000            18.0  \n2            7.400          0.000          0.000            18.0  \n3           14.000          0.000          0.000            17.0  \n4            1.800          0.000          0.000             1.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>Global_active_power</th>\n      <th>Global_reactive_power</th>\n      <th>target</th>\n      <th>Global_intensity</th>\n      <th>Sub_metering_1</th>\n      <th>Sub_metering_2</th>\n      <th>Sub_metering_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>25/1/2010</td>\n      <td>10:47:00</td>\n      <td>1.444</td>\n      <td>0.088</td>\n      <td>242.600</td>\n      <td>5.800</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14/11/2007</td>\n      <td>10:54:00</td>\n      <td>1.360</td>\n      <td>0.000</td>\n      <td>237.830</td>\n      <td>5.600</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>21/1/2007</td>\n      <td>14:19:00</td>\n      <td>1.758</td>\n      <td>0.406</td>\n      <td>243.950</td>\n      <td>7.400</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>18.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9/1/2009</td>\n      <td>18:14:00</td>\n      <td>3.358</td>\n      <td>0.000</td>\n      <td>239.730</td>\n      <td>14.000</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>17.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17/6/2009</td>\n      <td>23:43:00</td>\n      <td>0.428</td>\n      <td>0.076</td>\n      <td>242.830</td>\n      <td>1.800</td>\n      <td>0.000</td>\n      <td>0.000</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "       parents     has_nurs        form children     housing     finance  \\\n0  pretentious     critical  incomplete     more   less_conv      inconv   \n1   great_pret     improper  incomplete        2    critical      inconv   \n2        usual       proper   completed        1   less_conv      inconv   \n3  pretentious     improper      foster        3   less_conv      inconv   \n4        usual  less_proper      foster        2  convenient  convenient   \n\n          social       health      target  \n0    problematic     priority  spec_prior  \n1  slightly_prob  recommended  spec_prior  \n2  slightly_prob    not_recom   not_recom  \n3        nonprob    not_recom   not_recom  \n4    problematic     priority    priority  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>parents</th>\n      <th>has_nurs</th>\n      <th>form</th>\n      <th>children</th>\n      <th>housing</th>\n      <th>finance</th>\n      <th>social</th>\n      <th>health</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pretentious</td>\n      <td>critical</td>\n      <td>incomplete</td>\n      <td>more</td>\n      <td>less_conv</td>\n      <td>inconv</td>\n      <td>problematic</td>\n      <td>priority</td>\n      <td>spec_prior</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>great_pret</td>\n      <td>improper</td>\n      <td>incomplete</td>\n      <td>2</td>\n      <td>critical</td>\n      <td>inconv</td>\n      <td>slightly_prob</td>\n      <td>recommended</td>\n      <td>spec_prior</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>usual</td>\n      <td>proper</td>\n      <td>completed</td>\n      <td>1</td>\n      <td>less_conv</td>\n      <td>inconv</td>\n      <td>slightly_prob</td>\n      <td>not_recom</td>\n      <td>not_recom</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pretentious</td>\n      <td>improper</td>\n      <td>foster</td>\n      <td>3</td>\n      <td>less_conv</td>\n      <td>inconv</td>\n      <td>nonprob</td>\n      <td>not_recom</td>\n      <td>not_recom</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>usual</td>\n      <td>less_proper</td>\n      <td>foster</td>\n      <td>2</td>\n      <td>convenient</td>\n      <td>convenient</td>\n      <td>problematic</td>\n      <td>priority</td>\n      <td>priority</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Add missing variable names\n",
    "# specify target columns\n",
    "ad_var = ['age','workclass','fnlwgt','education',\n",
    "    'education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','target']\n",
    "ad.columns = ad_var\n",
    "nsr_var = ['parents', 'has_nurs', 'form', 'children', 'housing','finance', 'social', 'health','target']\n",
    "nsr.columns = nsr_var\n",
    "avl=avl.rename(columns={\"10\":'target'})\n",
    "pwr = pwr.rename(columns= {'Voltage': 'target'})\n",
    "display(ad.head())\n",
    "display(avl.head())\n",
    "display(pwr.head())\n",
    "display(nsr.head())"
   ]
  },
  {
   "source": [
    "#### nan value"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.read_csv('./data/adult.csv')\n",
    "print (ad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_var = ['age','workclass','fnlwgt','education',\n",
    "    'education-num','marital-status','occupation','relationship','race','sex','capital-gain','capital-loss','hours-per-week','native-country','target']\n",
    "ad.columns = ad_var\n",
    "display(ad.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " While we know from dataset that education and education-num are corresponded to each other,\n",
    "1. We could assign category and order, based on its value, to education column and drop education-num column\n",
    "2. 'fnlwgt' column could also be dropped since it is the final weight-- the number of people the census believes. Therefore, such column has no contibution to predition.\n",
    "3. sign 0 and 1 to target, which will make the work easier.\n",
    "'''\n",
    "ad = ad.drop(['education-num','fnlwgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "age [40 54 23 18 19 37 24 42 48 38 49 22 47 41 57 61 56 51 34 59 31 30 68 53\n 33 35 44 50 26 60 32 20 27 45 46 55 36 25 52 62 58 63 29 17 43 70 39 28\n 69 64 21 73 75 66 67 65 72 76 90 79 71 88 77 81 80 74 84 78 82]  \n\nworkclass [' Federal-gov' ' Private' ' Local-gov' ' Self-emp-inc' ' ?' ' State-gov'\n ' Self-emp-not-inc' ' Never-worked' ' Without-pay']  \n\nfnlwgt [544792 171924 133061 ...  33729 229803 312206]  \n\neducation [' Some-college' ' HS-grad' ' Bachelors' ' Masters' ' Doctorate'\n ' Prof-school' ' 7th-8th' ' Assoc-acdm' ' 11th' ' 12th' ' Assoc-voc'\n ' 10th' ' 9th' ' 5th-6th' ' 1st-4th' ' Preschool']  \n\neducation-num [10  9 13 14 16 15  4 12  7  8 11  6  5  3  2  1]  \n\nmarital-status [' Married-civ-spouse' ' Never-married' ' Separated' ' Divorced'\n ' Widowed' ' Married-spouse-absent' ' Married-AF-spouse']  \n\noccupation [' Farming-fishing' ' Transport-moving' ' Sales' ' Other-service'\n ' Adm-clerical' ' Tech-support' ' Exec-managerial' ' Prof-specialty' ' ?'\n ' Craft-repair' ' Handlers-cleaners' ' Machine-op-inspct'\n ' Protective-serv' ' Priv-house-serv' ' Armed-Forces']  \n\nrelationship [' Husband' ' Own-child' ' Unmarried' ' Not-in-family' ' Other-relative'\n ' Wife']  \n\nrace [' White' ' Black' ' Amer-Indian-Eskimo' ' Asian-Pac-Islander' ' Other']  \n\nsex [' Male' ' Female']  \n\ncapital-gain [    0 99999 15024  2580  3103  4787  5178  2009  4650  7688  4064  3411\n  4101  2653  4386  6497  5013  7298  1797 10520  3418  3325  8614  1506\n  2597 27828 14344  1848  2346   594  3674  2354  4865  2407  6849  2964\n  3137  3908  2977  2829 13550  3942  1151  2635   114 14084 10605  3781\n  9386  2885  5556  1055  2329  9562  5455  1424  2174 20051  7443  7430\n  2414  6418  2907  3887  4416  2961 15831 15020 34095  2463  6097   991\n  4508  2062  2050  3464  7896 41310  2936  7978  1409  4687]  \n\ncapital-loss [   0 1887 1902 1980 1848 1672 1602 2002 1977 1740 1617 2377 1408 2559\n 1974 2258 2174 1719  625 1504 2201 1579 1485 2179 3683 1092 1628 2339\n 4356 2754 2231 2246 1669 2042 2415 1564 1590  880 1876 1741 1258 1816\n 1762  323 2824 2001 2205  653 1573 2392 2051 2057 2129 2603 1594  974\n 1825 1721 2444 3004 1651]  \n\nhours-per-week [40 70 80 16 25 50 35 45 43 60 30 99 17 46 20 38 55 32 56 39 21 37 52 24\n 48 44 49 36 15 10  7 72  6 22 58 13 42 19 84 28 23 12  4 41  8 18 33 54\n 64  2 11 65 91 75 47 67 53  9 63 81 62 90 77 26  5  1 27 76 14 85  3 29\n 95 78 34 73 98 57 88 51 61]  \n\nnative-country [' United-States' ' ?' ' Columbia' ' Germany' ' Puerto-Rico' ' Mexico'\n ' Vietnam' ' Dominican-Republic' ' Canada' ' India' ' England' ' South'\n ' Guatemala' ' Haiti' ' China' ' Iran' ' Japan' ' Philippines'\n ' Nicaragua' ' Scotland' ' Cuba' ' Taiwan' ' France' ' Poland' ' Italy'\n ' El-Salvador' ' Laos' ' Ireland' ' Peru' ' Thailand' ' Greece'\n ' Jamaica' ' Hong' ' Cambodia' ' Yugoslavia' ' Portugal'\n ' Outlying-US(Guam-USVI-etc)' ' Honduras' ' Trinadad&Tobago' ' Ecuador']  \n\ntarget [' <=50K' ' >50K']  \n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 5000 entries, 0 to 4999\nData columns (total 15 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   age             5000 non-null   int64 \n 1   workclass       4703 non-null   object\n 2   fnlwgt          5000 non-null   int64 \n 3   education       5000 non-null   object\n 4   education-num   5000 non-null   int64 \n 5   marital-status  5000 non-null   object\n 6   occupation      4701 non-null   object\n 7   relationship    5000 non-null   object\n 8   race            5000 non-null   object\n 9   sex             5000 non-null   object\n 10  capital-gain    5000 non-null   int64 \n 11  capital-loss    5000 non-null   int64 \n 12  hours-per-week  5000 non-null   int64 \n 13  native-country  4901 non-null   object\n 14  target          5000 non-null   object\ndtypes: int64(6), object(9)\nmemory usage: 586.1+ KB\n"
     ]
    }
   ],
   "source": [
    "for col in ad:\n",
    "    print(col,ad[col].unique(), \" \\n\")\n",
    "# find out non-meaning cell and replace with nan\n",
    "# find our extreme numbers such as 0 and 9999 in int/float series and replace them with nan\n",
    "# here we find \" ?\" in ['occupation', 'workclass', 'native-country']\n",
    "# and extreme numbers 0 and 99999 in ['capital-loss','capital-gain']\n",
    "# In addition , in order to aviod mess, we use copy of dataframe instead operating directly on it.\n",
    "copy=ad.copy()\n",
    "copy = copy.replace({' ?': np.nan, 0: np.nan, 99999: np.nan, })\n",
    "copy.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.bar(list(copy['education'].value_counts().index),list(copy['education'].value_counts()),color='red')\n",
    "plt.bar(list(copy['education'].value_counts().index),list(copy['education-num'].value_counts()),color='blue')\n",
    "# blue layer completely covered the red layer with the same values\n",
    "# Therefore, we could prove that these two columns are exactly same and exclude one of them\n",
    "# In order to be more effeciency, we drop education column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. While we know from dataset and the figures showing above that education and education-num are corresponded to each other, we drop education column here.\n",
    "2. 'fnlwgt' column could also be dropped since it is the final weight-- the number of people the census believes. Therefore, such column has no contibution to predition.\n",
    "3. we only have hundreds of capital-gain and capital-loss record, these can be excluded. \n",
    "3. sign 0 and 1 to target, which will make the work easier.\n",
    "'''\n",
    "def incomeFixer(x):\n",
    "    if x == \" <=50K\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "# with same aim, we use copy_drop to drop some columns\n",
    "cp_dp=copy.copy()\n",
    "col = 'education'\n",
    "cp_dp[\"target\"] = cp_dp.apply(lambda x: incomeFixer(x['target']), axis=1)\n",
    "cp_dp = cp_dp.drop(columns=[col,'fnlwgt','capital-gain','capital-loss'])\n",
    "display(cp_dp.info())\n",
    "cp_dp.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_dp.columns"
   ]
  },
  {
   "source": [
    "From above, it shows that there are relatively small amount of missing values all from columns which are object types:\n",
    "\n",
    "    - Categorical data with text that needs encoded: ['workclass', 'marital-status', 'occupation','relationship', 'race', 'sex', , 'native-country']\n",
    "    - Categorical data that has null values: ['workclass','occupation','native-country']\n",
    "\n",
    "However, I do not want to simply dropna() to handle these missing values, there must be smarter ways such as Replace by most frequent value.\n",
    "\n",
    "we could also see that most of our columns are object while categorical, we could encode them using Integer Encoding/ One Hot Encoding/ Learned Embedding. \n",
    "\n",
    "But these how can we find the most effecient way to handle missing value and encode strings? \n",
    "\n",
    "1. We certainly need to practive on CategoricalImputer()\n",
    "\n",
    "2. For the encoding part, we could combine the inputer with encode to have a pipline for preprocessing.\n",
    "\n",
    "3. For extra credit, we could conbine these steps with a regular training-predicting process and use gradsearch to find out the best parameters and hyperparameters and try on BaseNEncoder to find the answer Later\n",
    "\n",
    "We could design unique **pipeline** for these preprocessing steps while along with the training processes using gridsearchCV to find our best parameters as well as best hyperparameters.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function help us to fillna\n",
    "categorical_missing = ['workclass','occupation','native-country']\n",
    "def impute_nan_most_frequent_category(DataFrame,ColName):\n",
    "    # .mode()[0] - gives first category name\n",
    "     most_frequent_category=DataFrame[ColName].mode()[0]\n",
    "    \n",
    "    # replace nan values with most occured category\n",
    "     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n",
    "     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n",
    "     DataFrame[ColName] = DataFrame[ColName + \"_Imputed\"]\n",
    "     DataFrame = DataFrame.drop([ColName + \"_Imputed\"], axis = 1)\n",
    "     return DataFrame\n",
    "\n",
    "for Columns in categorical_missing:\n",
    "    clnd = impute_nan_most_frequent_category(cp_dp,Columns)\n",
    "clnd.info()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['age','hours-per-week','education-num']\n",
    "categorical_features = ['workclass', 'marital-status', 'occupation','relationship', 'race', 'sex', 'native-country']\n",
    "y = cp_dp.target\n",
    "X = cp_dp.drop(columns=['target'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_cat', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "numeric_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_num', SimpleImputer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('categoricals', categorical_transformer, categorical_features),\n",
    "        ('numericals', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder = 'drop'\n",
    ")\n",
    "params = [{'classifier':[KNeighborsClassifier()],\n",
    "    'classifier__n_neighbors' : [1,3,5,10,20,30,50],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "s_scores = ['f1', 'accuracy']\n",
    "for score in s_scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    clf = GridSearchCV(\n",
    "        pipeline, params, scoring=score,cv=5,\n",
    "        n_jobs=-1,refit=callable)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"**pipeline**:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(params)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(\" \",clf.best_params_)\n",
    "    print(\"Best estimatpr found:\")\n",
    "    print(\" \",clf.best_estimator_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    target_names = ['class 0', 'class 1', 'class 2']\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Methodology"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Algorithms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There will be 5 fold cross-validation to select the hyperparameters via a gridsearch\nThe supervised machine learninf models are:\n [('knn', KNeighborsClassifier()), ('dtree', DecisionTreeClassifier()), ('svm', SVC())]\nThe methods to evaluate model performances are:  {'ACC': 'accuracy', 'FSC': 'f1'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('There will be 5 fold cross-validation to select the hyperparameters via a gridsearch')\n",
    "print('The supervised machine learninf models are:\\n', models)\n",
    "print('The methods to evaluate model performances are: ', s_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-b5d310a37887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m categorical_transformer = Pipeline(\n\u001b[1;32m      5\u001b[0m     [\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0;34m'imputer_cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'missing'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m'onehot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle_unknown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "numeric_features = ['age','hours-per-week','education-num']\n",
    "categorical_features = ['workclass', 'marital-status', 'occupation','relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_cat', SimpleImputer(strategy = 'constant', fill_value = 'missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "numeric_transformer = Pipeline(\n",
    "    [\n",
    "        ('imputer_num', SimpleImputer()),\n",
    "        ('scaler', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        ('categoricals', categorical_transformer, categorical_features),\n",
    "        ('numericals', numeric_transformer, numeric_features)\n",
    "    ],\n",
    "    remainder = 'drop'\n",
    ")\n",
    "params = [{'classifier':[KNeighborsClassifier()],\n",
    "    'classifier__n_neighbors' : [1,3,5,10,20,30,50],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}]\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocessing', preprocessor),\n",
    "        ('classifier', KNeighborsClassifier())\n",
    "    ]\n",
    ")\n",
    "s_scores = ['f1', 'accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define grid search parameters\n",
    "folds=5\n",
    "C = [1*(10**-5),1*(10**-4), 1*(10**-3),1*(10**-2), 1*(10**-1),1,10,1*(10**2),1*(10**3),1*(10**4),1*(10**5)]\n",
    "gamma = [1, 0.1, 0.01, 0.001, 0.0001]\n",
    "\n",
    "parameters=[\n",
    "    #knn\n",
    "    {'knn_n_neighbors' : [1,3,5,10,20,30,50],\n",
    "    'knn__weights': ['uniform', 'distance'],\n",
    "    'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    'knn__preprocess':[StandardScaler(),'passthrough'],\n",
    "    'knn__strategy': ['mean', 'median']},\n",
    "    #dtree\n",
    "    {'classifier':[DecisionTreeClassifier()],\n",
    "    'classifier__min_samples_split': range(2, 403, 10),\n",
    "    'classifier__criterion' : ['gini', 'entropy'],\n",
    "    'classifier__max_depth' : [2,4,6,8,10,12],\n",
    "    'classifier__strategy': ['mean', 'median']},\n",
    "    {'classifier':[SVC()],\n",
    "    'classifier__C': C, \n",
    "    'classifier__gamma': gamma, \n",
    "    'classifier__kernel': ['rbf','linear'],\n",
    "    'classifier__strategy': ['mean', 'median']}\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Trails"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for i, degree in enumerate([2, 9]):\n",
    "    N, train_lc, val_lc = learning_curve()\n",
    "\n",
    "    ax[i].plot(N, np.mean(train_lc, 1), color='blue', label='training score')\n",
    "    ax[i].plot(N, np.mean(val_lc, 1), color='red', label='validation score')\n",
    "    ax[i].hlines(np.mean([train_lc[-1], val_lc[-1]]), N[0], N[-1],\n",
    "                 color='gray', linestyle='dashed')\n",
    "\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_xlim(N[0], N[-1])\n",
    "    ax[i].set_xlabel('training size')\n",
    "    ax[i].set_ylabel('score')\n",
    "    ax[i].set_title('degree = {0}'.format(degree), size=14)\n",
    "    ax[i].legend(loc='best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness\n",
    "seed = 7\n",
    "for name, model in models:\n",
    "\tkfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "\tcv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n",
    "\tresults.append(cv_results)\n",
    "\tnames.append(name)\n",
    "\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "\tprint(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}